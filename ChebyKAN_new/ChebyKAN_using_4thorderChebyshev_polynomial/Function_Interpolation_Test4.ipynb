{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9dcab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from Cheby4KANLayer import Cheby4KANLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb43f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target function\n",
    "def target_function(x):\n",
    "    y = np.zeros_like(x)\n",
    "    mask1 = x < 0.5\n",
    "    y[mask1] = np.sin(20 * np.pi * x[mask1]) + x[mask1] ** 2\n",
    "    mask2 = (0.5 <= x) & (x < 1.5)\n",
    "    y[mask2] = 0.5 * x[mask2] * np.exp(-x[mask2]) + np.abs(np.sin(5 * np.pi * x[mask2]))\n",
    "    mask3 = x >= 1.5\n",
    "    y[mask3] = np.log(x[mask3] - 1) / np.log(2) - np.cos(2 * np.pi * x[mask3])\n",
    "\n",
    "    # add noise\n",
    "    noise = np.random.normal(0, 0.2, y.shape)\n",
    "    y += noise\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b389b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP and ChebyKAN\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x-1) # centralize the input\n",
    "\n",
    "\n",
    "class Cheby4KAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cheby4KAN, self).__init__()\n",
    "        self.cheby4kan1 = Cheby4KANLayer(1, 8, 8)\n",
    "        self.cheby4kan2 = Cheby4KANLayer(8, 1, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cheby4kan1(x)\n",
    "        x = self.cheby4kan2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc00cd91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     20\u001b[0m     optimizer_cheby4\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m     outputs_cheby4 \u001b[38;5;241m=\u001b[39m \u001b[43mcheby4_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     loss_cheby4 \u001b[38;5;241m=\u001b[39m criterion(outputs_cheby4, y_train)\n\u001b[1;32m     23\u001b[0m     loss_cheby4\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/software/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mCheby4KAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheby4kan1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheby4kan2(x)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data/software/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/qjw/ChebyKAN/Cheby4KANLayer.py:25\u001b[0m, in \u001b[0;36mCheby4KANLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputdim, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m )                                           \u001b[38;5;66;03m# shape = (batch_size, inputdim, self.degree + 1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m                                            \u001b[38;5;66;03m#reshapes and repeats 'x' to have shape (batch_size, inputdim, self.degree + 1)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m                                    \u001b[38;5;66;03m#apply inverse cosine function to 'x'\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Multiply by arange [0 .. degree]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marange \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m                                \u001b[38;5;66;03m#multiplies 'x' elementwise by the buffer 'arange'\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply cos\u001b[39;00m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msin()                                          \u001b[38;5;66;03m#apply sine function to 'x'\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation."
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "x_train = torch.linspace(0, 2, steps=500).unsqueeze(1)\n",
    "y_train = torch.tensor(target_function(x_train))\n",
    "\n",
    "# Instantiate models\n",
    "cheby4_model = Cheby4KAN()\n",
    "mlp_model = SimpleMLP()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_cheby4 = torch.optim.Adam(cheby4_model.parameters(), lr=0.01)\n",
    "optimizer_mlp = torch.optim.Adam(mlp_model.parameters(), lr=0.03)\n",
    "\n",
    "cheby4_losses = []\n",
    "mlp_losses = []\n",
    "\n",
    "# Train the models\n",
    "epochs = 200000\n",
    "for epoch in range(epochs):\n",
    "    optimizer_cheby4.zero_grad()\n",
    "    outputs_cheby4 = cheby4_model(x_train)\n",
    "    loss_cheby4 = criterion(outputs_cheby4, y_train)\n",
    "    loss_cheby4.backward()\n",
    "    optimizer_cheby4.step()\n",
    "\n",
    "    optimizer_mlp.zero_grad()\n",
    "    outputs_mlp = mlp_model(x_train)\n",
    "    loss_mlp = criterion(outputs_mlp, y_train)\n",
    "    loss_mlp.backward()\n",
    "    optimizer_mlp.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        cheby4_losses.append(loss_cheby4.item())\n",
    "        mlp_losses.append(loss_mlp.item())\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Cheby4KAN Loss: {loss_cheby4.item():.4f}, MLP Loss: {loss_mlp.item():.4f}')\n",
    "\n",
    "# Test the models\n",
    "x_test = torch.linspace(0, 2, steps=400).unsqueeze(1)\n",
    "y_pred_cheby4 = cheby4_model(x_test).detach()\n",
    "y_pred_mlp = mlp_model(x_test).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3910f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
